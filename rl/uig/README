12/7/16

investigation into scoring conundrum:

# original dictionary
cut -f1,3 additional/lexicon/lexicon | ../../scripts/scoredict.py -c uig.dev
Precision: 0.188762
Recall: 0.163334
F1: 0.175130

# tokenized
$ paste <(cut -f1 additional/lexicon/lexicon) <(cut -f3 additional/lexicon/lexicon | ~/projects/lorelei/unitok/scripts/tok.py) | ../../scripts/scoredict.py -c uig.dev 2> /dev/null
Precision: 0.317169
Recall: 0.422648
F1: 0.362389


# lowercased

cut -f1,3 additional/lexicon/lexicon | lowercase.py > uig.lex.lc
$ ../../scripts/scoredict.py -d uig.lex.lc -c uig.dev
Precision: 0.186832
Recall: 0.161861
F1: 0.173452

# lowercased and tokenized

$ paste <(cut -f1 additional/lexicon/lexicon) <(cut -f3 additional/lexicon/lexicon | ~/projects/lorelei/unitok/scripts/tok.py) | lowercase.py | ../../scripts/scoredict.py -c uig.dev 2> /dev/null
Precision: 0.305248
Recall: 0.406441
F1: 0.348650

# original dictionary transformed by seb script
python3 ../../../uyghur-dictprocessing/tools/norm_flat_uyghur_dict.py -i additional/lexicon/lexicon -o uig.lex.seb.norm
$ cut -f1,3 uig.lex.seb.norm | ../../scripts/scoredict.py -c uig.dev
Precision: 0.559165
Recall: 0.202905
F1: 0.297761

# original dictionary transformed by seb script and tokenized

$ for d in uig.lex.seb.norm; do paste <(cut -f1 $d) <(cut -f3 $d | ~/projects/lorelei/unitok/scripts/tok.py) | ../../scripts/scoredict.py -c uig.dev 2> /dev/null ; done
Precision: 0.583286
Recall: 0.217428
F1: 0.316774


# lowercased dictionary transformed
$ cut -f1,3 uig.lex.seb.norm | lowercase.py | ../../scripts/scoredict.py -c uig.dev
Precision: 0.519164
Recall: 0.188171
F1: 0.276224

# lowercased tokenized dictionary transformed

$ for d in uig.lex.seb.norm; do paste <(cut -f1 $d) <(cut -f3 $d | ~/projects/lorelei/unitok/scripts/tok.py) | lowercase.py | ../../scripts/scoredict.py -c uig.dev 2> /dev/null; done
Precision: 0.544633
Recall: 0.202905
F1: 0.295660


# modified seb script outputs normal seb analysis but also 'explanation' that doesn't include cross references, etc. that can then be 'unrolled' into instruction
# file, which can be used to generate modified dictionary
python3 ../../../uyghur-dictprocessing/tools/norm_flat_uyghur_instructions.py -i additional/lexicon/lexicon -o uig.lex.seb.adj.norm -e uig.lex.seb.adj.expl

# the dictionary in expl (compare to 29.77 above for plain transformation):
$  cut -f1-2 uig.lex.seb.adj.expl | ../../scripts/scoredict.py -c uig.dev
Precision: 0.200315
Recall: 0.133656
F1: 0.160333

# the dictionary in expl with english tokenized first (compare to the 16.0 above)
$ paste <(cut -f1 uig.lex.seb.adj.expl) <(cut -f2 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py ) | ../../scripts/scoredict.py -c uig.dev
Precision: 0.376272
Recall: 0.365818
F1: 0.370971


paste <(cut -f1 uig.lex.seb.adj.expl) <(cut -f2 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py )  <(cut -f3 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py ) | python3 ../../scripts/unrollexpl.py --no-insert -o uig.lex.seb.adj.expl.tok.codes.ni 2> unroll.ni.stderr

# the original filtered for cross-reference dictionary scored
cut -f1-2 uig.lex.seb.adj.expl.tok.codes.ni | ../../scripts/scoredict.py -c uig.dev
Precision: 0.383282
Recall: 0.367712
F1: 0.375336


# the effect of the instruction file scored
$ ../../scripts/simplifydict.py -d <(cut -f1-2 uig.lex.seb.adj.expl.tok.codes.ni) -i <(cut -f4 uig.lex.seb.adj.expl.tok.codes.ni) | ../../scripts/scoredict.py -c uig.dev
Precision: 0.616099
Recall: 0.167544
F1: 0.263445


             p   r  f
orig untok  .19 .16 .18
seb  untok  .56 .20 .30
orig   tok  .32 .42 .36
seb    tok  .58 .22 .32

tok drastically affects orig, doesn't affect seb

let's look at cases where orig tok >> orig untok and see what seb untok did in these cases as well
paste <(cut -f1 additional/lexicon/lexicon) <(cut -f3 additional/lexicon/lexicon | ~/projects/lorelei/unitok/scripts/tok.py) | ../../scripts/scoredict.py -c uig.dev --persent -o orig.tok.dev.persent
cut -f1,3 additional/lexicon/lexicon | ../../scripts/scoredict.py -c uig.dev --persent -o orig.untok.dev.persent
cut -f1,3 uig.lex.seb.norm | ../../scripts/scoredict.py -c uig.dev --persent -o seb.untok.dev.persent
paste <(cut -f1 orig.untok.dev.persent) <(cut -f1 orig.tok.dev.persent) <(cut -f1,3 seb.untok.dev.persent) <(cut -f2 orig.untok.dev.persent) <(cut -f2 orig.tok.dev.persent) <(cut -f2 seb.untok.dev.persent) | awk '$(12)>$6{print}' | head

p 0.000000 r 0.000000 f 0.000000	p 0.500000 r 0.285714 f 0.363636	p 1.000000 r 0.142857 f 0.250000	[[[The current script will be lost .]]]	[[[current.///present,]]]	[[[.///,///present///current]]]	[[[current]]]
period tokenized off in both cases; errant punctuation reward
p 0.000000 r 0.000000 f 0.000000	p 0.125000 r 0.166667 f 0.142857	p 0.500000 r 0.166667 f 0.250000	[[[% s missing name or label]]]	[[[wrong,///or.///mistaken.///erroneous,]]]	[[[mistaken///erroneous///.///wrong///,///or]]]	[[[erroneous///or]]]
period tokenized off
p 0.000000 r 0.000000 f 0.000000	p 0.100000 r 0.250000 f 0.142857	p 0.000000 r 0.000000 f 0.000000	[[[That won't happen .]]]	[[[matters,///(demonstrative///that///things,///adjective).///business.///hundred.]]][[[hundred///adjective///(///.///)///that///work///demonstrative]]]	[[[appearance///activities///he]]]
case issue, errant punc reward
p 0.250000 r 0.200000 f 0.222222	p 0.400000 r 0.400000 f 0.400000	p 1.000000 r 0.200000 f 0.333333	[[[I don't speak Japanese .]]]	[[[in///Japanese///the///way.]]]	[[[.///Japanese///the///way///in]]]	[[[Japanese]]]
errant punc reward
p 0.000000 r 0.000000 f 0.000000	p 0.333333 r 0.333333 f 0.333333	p 1.000000 r 0.166667 f 0.285714	[[[People from Madrid are weird .]]]	[[[strange,///surprising,///weird.]]]	[[[weird///,///strange///.///surprising]]]	[[[weird]]]
tok issue, errant punc reward
p 0.166667 r 0.166667 f 0.166667	p 0.230769 r 0.500000 f 0.315789	p 1.000000 r 0.500000 f 0.666667	[[[Are they all the same ?]]]	[[[someone.///form)///they.///definite///the///(in]]]	[[[,///the///same///common///a///single///they///.///unique]]]	[[[they///same///the]]]
tok issue, 
p 0.200000 r 0.125000 f 0.153846	p 0.200000 r 0.250000 f 0.222222	p 0.333333 r 0.125000 f 0.181818	[[[He would be glad to hear that .]]]	[[[happy,///(demonstrative///that///joyful.///adjective).]]]	[[[adjective///joyful///(///.///,///)///that///happy///demonstrative]]]	[[[delighted///with///that]]]
synonym miss, errant punc reward
p 0.000000 r 0.000000 f 0.000000	p 0.080000 r 0.250000 f 0.121212	p 0.200000 r 0.125000 f 0.153846	[[[" Thank you ." " You're welcome ."]]]	[[[all///welcome!///a///not///retort///you're///thanks).///Mercy.///it!///(as///[REL]///Divine///at///mention///to///don't]]]	[[[it///as///to///)///you///retort///at///!///re///don///t///thank///not///welcome///a///(///.///mention///'///thanks///all]]]	[[[don't///mention///thank///you///it!]]]
tok issue, errant punc reward
p 0.000000 r 0.000000 f 0.000000	p 0.500000 r 0.400000 f 0.444444	p 1.000000 r 0.200000 f 0.333333	[[[I don't know him .]]]	[[[I,///me.]]]	[[[.///,///I///me]]]	[[[I]]]
tok issue, errant punc reward
p 0.000000 r 0.000000 f 0.000000	p 0.250000 r 0.400000 f 0.307692	p 0.200000 r 0.200000 f 0.200000	[[[Yesterday we had fun .]]]	[[[entertainment,///yesterday.///amusement,///fun.]]]	[[[entertainment///,///amusement///fun///.///yesterday]]]	[[[in///past///the///fun///recent]]]
capitalization, tok issue, errant punc reward

Based on this I will:
a) run tokenizer and lowercaser on dev
b) internal to scoring script eliminate all tokens in reference or lookups that are 100% punctuation

the motivation is that those shouldn't matter in coverage decisions

so now how is orig vs seb (toklc)?

$ for i in additional/lexicon/lexicon uig.lex.seb.norm; do paste <(cut -f1 $i) <(cut -f3 $i | ~/projects/lorelei/unitok/scripts/tok.py | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc 2> /dev/null; done
Precision: 0.439227
Recall: 0.415736
F1: 0.427158

Precision: 0.685294
Recall: 0.276919
F1: 0.394447


      p   r   f
orig .44 .42 .43
seb  .69 .28 .39

big recall hit. is it appropriate?
hmm:
$ paste <(cut -f1 orig.tok.lc.nopunc.dev.persent) <(cut -f1,3 seb.tok.lc.nopunc.dev.persent) <(cut -f2 orig.tok.lc.nopunc.dev.persent) <(cut -f2 seb.tok.lc.nopunc.dev.persent) | awk '$(12)<$6{print}' | wc
      40     930    8808
$ paste <(cut -f1 orig.tok.lc.nopunc.dev.persent) <(cut -f1,3 seb.tok.lc.nopunc.dev.persent) <(cut -f2 orig.tok.lc.nopunc.dev.persent) <(cut -f2 seb.tok.lc.nopunc.dev.persent) | awk '$(12)>$6{print}' | wc
     413    9155   78803
$ paste <(cut -f1 orig.tok.lc.nopunc.dev.persent) <(cut -f1,3 seb.tok.lc.nopunc.dev.persent) <(cut -f2 orig.tok.lc.nopunc.dev.persent) <(cut -f2 seb.tok.lc.nopunc.dev.persent) | awk '$(12)==$6{print}' | wc
     236    4794   31471

Turns out the f measure was being calculated corpus wide on matches: reformulated to calculate matches sentence-wide only. Now things make more sense:

lowercased tokenized base

$ for i in additional/lexicon/lexicon uig.lex.seb.norm; do paste <(cut -f1 $i) <(cut -f3 $i | ~/projects/lorelei/unitok/scripts/tok.py | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc 2> /dev/null; done
Precision: 0.235560
Recall: 0.222962
F1: 0.229088

Precision: 0.522941
Recall: 0.211314
F1: 0.300999

      p   r   f
orig .24 .22 .23
seb  .52 .21 .30

now just the explainable part of orig, tok and lc:
paste <(cut -f1 uig.lex.seb.adj.expl) <(cut -f2 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc 
$ paste <(cut -f1 uig.lex.seb.adj.expl) <(cut -f2 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc 
Precision: 0.231540
Recall: 0.164725
F1: 0.192500

(note: worse, which is the proper direction)
and the reduced explainable part once more filtering is done:
paste <(cut -f1 uig.lex.seb.adj.expl) <(cut -f2 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py )  <(cut -f3 uig.lex.seb.adj.expl | ~/projects/lorelei/unitok/scripts/tok.py ) | python3 ../../scripts/unrollexpl.py --no-insert -o uig.lex.seb.adj.expl.tok.codes.ni 2> unroll.ni.stderr

$ paste <(cut -f1 uig.lex.seb.adj.expl.tok.codes.ni) <(cut -f2 uig.lex.seb.adj.expl.tok.codes.ni | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc
Precision: 0.239085
Recall: 0.161398
F1: 0.192706

$ ../../scripts/simplifydict.py -d <(cut -f1-2 uig.lex.seb.adj.expl.tok.codes.ni) -i <(cut -f4 uig.lex.seb.adj.expl.tok.codes.ni) -o uig.lex.seb.adj.expl.tok.simplified
$ paste <(cut -f1 uig.lex.seb.adj.expl.tok.simplified) <(cut -f2 uig.lex.seb.adj.expl.tok.simplified | lowercase.py) | ../../scripts/scoredict.py -c uig.dev.tok.lc
Precision: 0.499602
Recall: 0.149037
F1: 0.229586


             p   r   f
orig        .24 .22 .23
seb         .52 .21 .30
orig-simple .24 .16 .19
seb-simple  .50 .15 .23

as expected, the simplification throws away a lot of good stuff but the effect of sebastian's code is consistent: it raises precision without affecting recall much.

now how about intrinsic measures (label accuracy) on the dev dict corpus?
we treat the transformation from orig to seb as an application of a label to the orig words. 

NOTE: dict corpus split into train/dev/test has been tokenized but NOT lowercased; may need to redo!!

$ pwd
/Users/jonmay/projects/lorelei/lexicon/rl/uig/uig.lex.seb.adj.expl.tok.codes.ni.exp
seb = 100, since this is by definition. (identity)
$ ../../../scripts/scoretagger.py -i dev.labels -r dev.labels
1.00 5983 5983
orig = always K
$ tr 'KSD' 'K' < dev.labels | ../../../scripts/scoretagger.py -r dev.labels 
0.48 2866 5983

most_freq: first data baseline: take the train side of the corpus and determine most frequent decision per token.
 ../../../../scripts/most_freq_dict2inst.py --tt ../train.text --tl ../train.labels --st ../dev.text -o dev.hyp
$ ../../../../scripts/scoretagger.py -i dev.hyp -r ../dev.labels | tee dev.score
0.88 5266 5983

extrinsic of the most_freq?
 ../../../../scripts/most_freq_dict2inst.py --tt ../train.text --tl ../train.labels --st ../all.text -o all.hyp
 ../../../../scripts/simplifydict.py -d <(cut -f1-2 ../../uig.lex.seb.adj.expl.tok.codes.ni) -i all.hyp -o all.dict.most_freq
 $ paste <(cut -f1 all.dict.most_freq ) <(cut -f2 all.dict.most_freq | lowercase.py) | ../../../../scripts/scoredict.py -c ../../uig.dev.tok.lc 
Precision: 0.430593
Recall: 0.151890
F1: 0.224565

great, so we can show a bit of a relationship between intrinsic and extrinsic measures

             extrinsic f       seb tag accuracy
orig-simple    .19                .48
seb-simple     .23               1.00
mf-simple      .22                .88

Now, can we learn a better seb tag accuracy (e.g. using the feed forward approach) and does it translate to similar extrinsic f?
If not, back to the drawing board; we likely have a fatal flaw somewhere. If so, then we can abandon seb tag accuracy optimization and try to directly optimize extrinsic f.

back on hpc, make tagger able to generate labels, generate 'em
$ pwd
/auto/rcf-40/jonmay/projects/tensorflow/tagger
$ python tagger.py --tt train.text --tl train.labels --dt dev.text --dl dev.labels --st dev.text -n 3 -w 5 -l 1 -e 20 -b 50 -E 5000 -p 1000 --relu -o dev.n3.w5.k1.e20.b50.E5000.p50.relu.hyp
Dev accuracy = 0.9514 = 5692/5983
to do extrinsic need to decode the whole thing though, so:
$ python tagger.py --tt train.text --tl train.labels --dt dev.text --dl dev.labels --st all.text -n 3 -w 5 -l 1 -e 20 -b 50 -E 5000 -p 1000 --relu -o all.n3.w5.k1.e20.b50.E5000.p50.relu.hyp
(note it is test on train. also note i don't save model right now so it's slightly different. also sometimes it doesn't converge properly so i restart. that's kind of bad...)

back here:
$ pwd
/Users/jonmay/projects/lorelei/lexicon/rl/uig/uig.lex.seb.adj.expl.tok.codes.ni.exp/n3.w5.k1.e20.b50.E5000
$ scp hpc:projects/tensorflow/tagger/all*.hyp .
all.n3.w5.k1.e20.b50.E7500.relu.hyp 
$ ../../../../scripts/simplifydict.py -d <(cut -f1-2 ../../uig.lex.seb.adj.expl.tok.codes.ni) -i all.n3.w5.k1.e20.b50.E7500.relu.hyp -o all.dict
$ paste <(cut -f1 all.dict ) <(cut -f2 all.dict | lowercase.py) | ../../../../scripts/scoredict.py -c ../../uig.dev.tok.lc 
Precision: 0.506883
Recall: 0.148800
F1: 0.230062
             extrinsic f       seb dev tag accuracy
orig-simple    .19                .48
seb-simple     .23               1.00
mf-simple      .22                .88
ff-simple      .23                .95

validated!

next, change ff code to optimize on scoredict score. also, add some wizardry to avoid occasional bad convergence.

12/20/16

working on getting that code to do the right thing. ready for testing. (hpc projects/tensorflow/tagger/rltagger.py)
current issues:

* get code working
* looks like it's really slow to generate tag set with proper number of tokens per line (reshape issue)
* i wonder: will there be oversplitting? should probably try to minimize entries along with everything else...